\documentclass[a4paper,BCOR=10mm]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{caption}

\usepackage{natbib}
\usepackage{algorithm,algorithmic}

\usepackage{multirow}

\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[begintext=``, endtext='']{quoting}
\SetBlockEnvironment{quoting}
\SetBlockThreshold{1} 

%\renewcommand{\baselinestretch}{1.5}

\newcounter{quotecount}
\newcommand{\countquote}[1]{\vspace{1cm}\refstepcounter{quotecount}%
     \parbox{9cm}{#1}\hspace*{1cm}(Excerpt \arabic{quotecount})\\[1cm]}

\renewcommand{\mkcitation}[1]{#1}
\renewcommand{\mkblockquote}[4]{\enquote{#1}#2\ifterm{\relax}{#3}#4}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%\renewcommand{\baselinestretch}{1.5}


\begin{document}

\begin{titlepage}
\pagenumbering{Roman} 
\begin{center}
 
\Large\textbf{\\
University of Heidelberg}
\vspace{2cm}

\Huge\textbf{
Abstractive Timeline Summarization
}

\vspace{10cm}

\normalsize
Masters Thesis in Computational Linguistics\\
submitted by\\
\vspace{0.5cm}
\Large\textbf{Julius Steen}\\
\normalsize
\vspace{0.5cm}
born in Stadthagen (Germany)\\
\normalsize
Matr-Nr.: 3219620 \\
\vspace{0.5cm}
\Large\textbf{May 2018} \\
\normalsize

\newpage

\normalsize
This Masters Thesis has been carried out by Julius Steen at the\\
Institut für Computerlinguistik in Heidelberg\\
under the supervision of\\
Dr. Katja Markert
\vfill
\end{center}

\newpage
\noindent
\Large\textbf{Abstract}\\
\\

Timeline summarization (TLS) is the task of taking a set of documents related to some large incident, for example a military conflict or natural disaster, and returning summaries for the most important events from that incident, aranged along a timeline. Each event is tagged with a timestamp, usually the day that it has occured.
Almost all prior work in the area of TLS has thus far focused on directly extracting intersting sentences from the original documents, an approach that is called extractive summarization. A more human-like approach to summarization is abstractive summarization which generates novel sentences to form a summarization. While many abstractive systems have been proposed for generic summarization tasks, we are not aware of any that have been used for TLS.
In this thesis we present an abstractive approach to timeline summarization that builds on existing systems for multi-document summarization. We adapt the system to better handle and take advantage of the special properties of TLS and charaterise the system in a general framework for abstractive TLS. We then evaluate this modified system and compare it to the original, unmodified system on two large copora.
We also conduct an annotation study to determine the lingustic quality of the generated summaries.


\newpage
\noindent
\Large\textbf{Zusammenfassung}\\
\\

\newpage
\normalsize
\tableofcontents

\end{titlepage}

\pagenumbering{arabic} 


\chapter{Introduction}

Many important events are not contained within a small timeframe but span multiple weeks, month or even years. In these long timespans many small events occur that contribute the overall development of the larger event. Often information about these incidents is spread out across a large number of different articles. This can make it difficult for users wanting to inform themselves to get an overview of the sequence of events. Many publications offer a solution to the problem by creating timelines of the events. %(EXAMPLES!)
A timeline lists the dates which are important to the event (or which the timeline creator cosinders important) along with brief summaries of the dates.
While such timelines are very useful, they must be created by hand and may not be available for all interesting topics and even if there are timelines available, they might not cover the period of time interesting to the reader.
Consequently there has been a number of attempts to generate such timelines automatically from large corpora of news articles pertaining to an event. See section \ref{sec:prior-work} for an overview.

As \citet{markert} note, the task of TLS is closely related to Multi Document Summarization.
A Multi Document Summarization is a system that takes a number of documents pertaining to some common topic and outputs a summary of their content that meets some length constraint. Generally, there are two classes of MDS systems, abstractive systems and extractive systems \citep{a-brief-survey}. Extractive systems work by selecting a subset of sentences from the original documents that best represent their content. This is the approach that has been employed by the mayority of TLS systems so far.
There are, however, inherent limits to the performance of purely extractive systems. While a recent study \citep{hirao+nishino} using an oracle of extractive summaries to determine an upper bound for extractive summarization has shown that there is still room for improvement of ROUGE \citep{lin} scores, another study by \citet{ceylan+mihalcea} suggest achieving meaningful improvements may be difficult. Their approach exhaustively generates all possible extractive summaries for documents in multiple domains and generates a probability density function from a histogram of ROUGE scores. They find that both the basic lead baseline and the realtively old TextRank \citet{textrank} system already fall into the 99th percentile of the function.

In comparison, abstractive summarization systems work much closer to the way we would expect a human to summarize a text. Instead of just selecting the sentences from the original document, they construct new sentences based on the information from the input documents. While they are not limited in the same way as extractive summarization systems, they are more difficult to design. A perfect abstractive system would arguably require almost human-like text understanding, in addition to the ability to determine relevance and to generate readable and coherent text based on that understanding. However, a number of abstractive systems has been proposed that tackle the task, for example by not constructing new sentences from scratch, but by merging sentences from the corpus and deleting unimportant constituents. To our knowledge, no such system has thus far been applied to TLS. Considering the availability of such systems for summarization and the limits of extractive summarization, it is interesting to investigate the application of such systems to the problem of timeline summarization.

A very basic approach to introduce abstractive timeline summarization is to take an existing abstractive MDS system and couple it with an algorithm that determines important dates within an event. This process is called date selection. We elaborate on this idea in section \ref{sec:tls-as-mds}. This approach, however, ignores the temporal aspect of TLS, which has previously shown to be important to the problem. For example, \citet{markert} have shown that temporalizing similarity functions used in an extractive summarization system can improve over just using the MDS system outright.
We thus structure our investigation into abstractive TLS as follows:
Starting from a state-of-the-art abstractive summarization system we first apply it to the TLS task without modifications, in a pipeline with a date-selection algorithm.
We then derive a general frameworks for abstractive TLS based on this original system and propose a number of adaptions to take advantage of the special structure of the TLS task.
Both the unmodified system and the adapted version are evaluated on two timeline summarization corpora. We also conduct an annotation study to determine the linguistic quality, which abstractive summarization systems, unlike their extractive counterparts, can not guarantee even on sentence level.


\chapter{Prior Work}

\section{Prior Work in Abstractive Document Summarization}

There is a great variety of models for abstractive summarization. One common approach is to combine existing sentences by fusing them or deleting parts. Usually this is is combined with some kind of learned function that determines which deletions lead to good summaries.
For example, \citet{almeida} work on dependency trees and solve an ILP to determine both which sentences should be included in the summarization and which nodes can be deleted based on a learned scoring function. They use a multi-task learning setup to learn appropriate scores using both corpora for extractive summarization and sentence compression to increase available training data. Similarly \citet{berg-kirkpatrick} work on parse trees and also solve an ILP to jointly choose sentences to extract and nodes to discard from a sentence based on scores learned on a training set.
\citet{bing} also work on parse trees but combine NPs and VPs from different sentences if they are similar enough. They select the best combinations based on salience of the included phrases.

Another approach to this kind of summarization, using the original documents to form new sentences, is based around graph modelling. \citet{filippova} suggests a model for headline generation that is based around finding shortest paths in a word graph derived from a set of sentences.
Each node is one of the words from the original sentences and directed edges indicate adjacency of two words in at least one of the sentences. Edges are weighted so that more frequently cooccuring words are connected by shorter edges. By running a k-shortest path algorithm, \citeauthor{filippova} generates a set of candidate summaries, which are then reweighted according to their length.
\citet{banerjee}, which we base our investigation on, builds on this approach and integrates this method in an abstractive multi document summarization, by first clustering the sentenences of the input documents and then using the graph compression method to generate a large number of candidate summary sentences for each cluster. They then formulate an ILP to select the most salient sentences for inclusion in the summary. See section \ref{sec:mds-baseline} 

The idea of building graphs from sentences has also been explored in other domains, for example \citet{opinosis} extract short summaries from highly opinated sentences from product reviews by using a similar method.

%OPINOSIS, that acyclic method!


Finally, a very recent avenue of research is the use of neural networks for abstractive summarization. A typical architecture is to use an encoder-decoder architecture with some kind of recurrent neural network, for example an LSTM. See \citet{nallapati, rush} for examples of such approaches.
While many early models were limited to summarize only short passages of texts, recent developments combining this architecture with methods from reinforment learning have substantially increased performance of neural summarizers for longer summaries. \citet{paulus}



\section{Prior Work in Timeline Summarization}

One of the earliest approaches to timeline summarization, which still provides a very strong baseline in many cases (see, for example, the evaluation of \citet{markert}), are the measures proposed by \citet{chieu}.
\citeauthor{chieu} proposes that the importance of a sentence for a timeline can be determined by two measures, \textit{interest} and \textit{burstiness}.
Interest is the measure of overall similarity of a sentence $s$ to those sentences in the corpus, which lie in a specified window around the date of $s$.
Burstiness measures whether a sentence reports an event that is strongly clustered around one specific date in the corpus, instead of spread out over the whole extent of the corpus.


\citet{yan-trans} model the summarization as a graph centrality problem. For each date, two separate graphs, are constructed: a local biased graph, that encodes the similarities of sentences refering to that date and a global biased graph that encodes the similarities of the sentences of that date (''intra-date similarities'') to all sentences outside that date (''inter-date similarities'').
They then use the DivRank \citep{divrank} algorithm to determine centrality scores for each sentence within one date for each graph, which are used to tank the sentences.
A final rank for each sentence is computed by a weighted average of the ranks obtained from both graphs. To determine which dates should be included in the timeline, they give preference to sentences from dates which have been mentioned often in their corpus. As they themselves note, this corresponds to the notion of burstiness introduced by \citeauthor{chieu}.

\citet{multimodal} have proposed a multimodal approach to timeline summarization by integrating image information. They cast the task as a collaborative filtering task. During training, they construct a matrix of textual features and image embeddings derived from a deep neural network for each sentence in the corpus, including a column of their rouge scores. They then factorize the matrix into two embedding matrices, one for sentences and one for the features, which together approximate the original matrix. During prediction the rouge column of a sentence is left empty and a new embedding is computed for the sentence. By then multiplying that embedding with the feature embedding for rouge, they can derive a score for each sentence. They then include the sentences with the highest scores and order them chronologically.




\chapter{Abstractive Timeline Summarization as Abstractive Multi-Document Summarization}

\section{Overview}

Timeline summarization is closely related to the field of multi-document summarization. As mentioned in the introduction already, any multi-document summarization system can be directly used for timeline summarization by first selecting a set of dates the timeline should include and then summarizing the set of documents that were published on that date.
While this procedure is bound to ignore the global aspects of timeline summarization, as each date summary is created in isolation, it is a convinient baseline for the problem.

However, while there are many different approaches to abstractive multi-document summarization, not all are immediatly suited to the nature of current timeline summarization tasks. Neural methods, while recently successfull in abstractive document summarization (CITE!), usually require a large ammount of training date, which is difficult to obtain given the limited number of reference timelines in current timeline corpora.
We thus focus on methods that work in an unsupervised way.

We opt to extend the work of \citep{banerjee} for two reasons:

\begin{enumerate}
\item{It does not require any training or parameter tuning}
\item{It can easily be decomposed into contained subproblems, which can independently be adapted to the challenge of timeline summarization}
\end{enumerate}.


\section{Multi-Document Baseline System} \label{sec:mds-baseline}

The reference system of \citet{banerjee} we have choosen for our task generates summaries in three steps. The clustering phase, the sentence generation phase and the selection phase. In the clustering phase similar sentences of the input documents are grouped into clusters. In the second step a graph-based compression algorithm is used on the clusters and novel candidate sentences are generated to summarize the cluster content. In the last step, sentences are scored for informativeness and linguistic quality and an optimal subset of non-redundant candidate sentences is choosen as a summary for the document. In the rest of this section we describe the above mentioned steps in more detail and also mention any decisions we made during our reimplementation of their system where the original documentation was unclear.
While \citeauthor{banerjee} suggest multiple variations to their system, we only reimplemented their best performing configuration. Note, that we leave out the sentence ordering step of the original system, because of the tearse structure of most timelines, where most events are described in only one sentence.

\subsection{Clustering}  \label{sec:baseline-clustering}

The clustering phase is divided in two distinct steps. In the first step, \citeauthor{banerjee} select what they call a most important document $D_{imp}$. This document is expected to contain the most important content of the input document set and is used to guide the actual clustering process. The second step is the assignment of sentences to appropriate clusters.

The best performing method for identifying the most important document on an evaluation conducted on DUC data is what \citeauthor{banerjee} call document set similarity.
The document set similarity of a document $d_i \in D$, is the cosine similarity between some vector representation for $d_i$ and the representation for the concatenation of all documents in $D$.
Following \citeauthor{banerjee} we represent each document as a tf-idf vector, where we fit the document frequency on the documents of the input document set.

In the clustering phase, we initialize one cluster for each sentence $s_j \in D_{imp}$. For all sentences $s_i \in D \setminus D_{imp}$ from other documents we iterate over each sentence of $D_{imp}$ and find $\argmax_{s_j} \text{CosSim}(s_i, s_j)$. The sentence is then added to the cluster induced by sentence $s_j$, as long as the similarity is above a certain level, to exclude sentences $s_i$ from the final clustering that are irrelevant to the summarization.
Again, we follow \citeauthor{banerjee} and reject all sentences which have a similarity of less than $0.5$ to their most similar sentence.

The last step of the clustering method is to filter out any unimportant clusters. This is achieved by eliminating all clusters which do not have at least $\frac{|D|}{2}$ members.


\subsection{Candidate Generation} \label{sec:baseline-generation}

The candidate generation step uses the method of \citet{fillipova}, which is based around the idea of summarizing sentences being paths in a graph of words and their adjaceny relations in the summarized sentences.

\citeauthor{fillipova} incrementally grows a directed graph of word adjanceny relations. Each word is represented as a tuple of surface form and POS tag. When a sentence is added to the graph, we test for each word of the newly added sentence if it matches one of the vertices in the graph. This is done in four phases, for four groups of words:

\begin{enumerate}
\item{Content words, which have exactly one matching node in the graph}
\item{Content words, which have more than one matching node in the graph, or which occur multiple times in the sentence}
\item{Stopwords and punctuation} % Punct not in fillipova!
\end{enumerate}

For ambigous nodes, the precedessors and succesors of the candidate nodes and the neighbours word to be mapped are compared and the word is mapped to the node with the largest overlap.
Similarly, stopwords and punctuation are only mapped to a node, if there is at least one word in their immediate neighbourhood, that is also a graph neighbour of the candidate node.

If no match can be found for a node, a new node is introduced to the graph. Note, that each node can only be mapped to once.

After the mapping and optional creation of new nodes, edges are introduced between every two nodes that two neighbouring words were mapped to.

Finally, we introduce edges from a special start node to the node of the first word in the sentence and from the node of the last word to a special terminal node.

In order to find summarizations from this graph, one must find loopless paths from the start node to the terminal node. \citeauthor{fillipova} then uses a k-shortest path algorithm to select candidate sentences. Note, that while \citeauthor{banerjee} follows the same approach, it is unclear, if and how they weigh edges for frequency and importance, which is an essential part of the generation process for \citeauthor{fillipova}.
In our reimplementation we choose the following conversion function proposed by \citeauthor{fillipova} to weight an edge based on its frequency $\text{freq}$:

\begin{displaymath}
w(e_{ij}) = \frac{\text{freq}(i) + \text{freq}(j)} {\text{freq}(i) * \text{freq}(j) * (\sum_{s \in C} \text{diff}(s, i, j)^{-1})}
\end{displaymath}

Where diff is a function that rewards edges between nodes that are often close in sentences. It is defined in terms of the position $\text{pos}(s, i)$ of the word in sentence $s$ that was mapped to vertex $v_i$.
The function is designed to reward edges which connect words which are often close by in the source sentences.

\begin{displaymath}
\text{diff}(s, i, j) = \begin{cases}
\text{pos}(s, j) - \text{pos}(s, i) & \text{if }\text{pos}(s, i) < \text{pos}(s, j) \\
0 & \text{else}
\end{cases}
\end{displaymath}

After the k-shortest paths have been obtained, two heuristics are applied to remove ungramatical sentences:

\begin{itemize}
    \item {Sentences with fewer than $8$ words are removed}
    \item {Sentences which do not have a verb  are removed}
\end{itemize}

Following \citet{banerjee} we retain only the 200 shortest paths of each graph for computational reasons.

In our reimplementation we use the algorithm of \citet{kshortestpath} as implemented in the networkx library \citep{networkx} to generate the $k$-shortest paths. Additionally we introduce another contraint to only include the 20 largest clusters at all steps to keep solving the ILP problem feasible.

\subsection{Scoring}

Given the set of generated sentences, we need to select the best candidates to be included in the final summary. \citet{banerjee} use two features to measure the quality of a sentence, linguistic quality and informativeness. This setup is supposed to balance readability of the sentences with their informative content.

Linguistic quality is measured by a trigram language model. Given such a model $P$ and a sentence $s = w_1, ..., w_n$ the lingistic quality $LQ(s)$ is defined as follows.
\begin{align*}
LQ(w_1, ..., w_n) = \frac{1}{1 - (\frac{1}{L} \log \prod_{i > 3}^{n} P(w_i | w_{i - 1}, w_{i - 2}))}
\end{align*}

Informativeness $I$ is measured by computing TextRank \citet{textrank} keyword scores and summing them over all tokens of a sentence.
TextRank scores are computed by applying the PageRank \citet{pagerank} algorithm to a an unweighted graph of the content words of the input sentences, in which those words that cooccur within a certain window of text are connected.
In our implementation we let the window cover the entire sentence.
Note that the TextRank scores are computed individually for each cluster, instead of globally for the entire document collection.

The final score is computed as $\text{score}(s) = I(s) * LQ(s)$. Note that we have slightly altered the original function by leaving out the division by length in the original formulation. This takes into account the different length constraints of our TLS setup and the DUC setup, which is used by \citeauthor{banerjee}. While DUC has a length limit in tokens, we instead limit the number of sentences for each entry.
This means it becomes less attractive to formulate many small sentences, instead rewarding the formulation of longer sentences.


\subsection{Selection} \label{sec:baseline-selection}

For selection our baseline system uses an Integer Linear Program (ILP), which is widely employed in summarization tasks.
The objective is to optimize the following function:

\begin{displaymath}
\sum_{i = 1}^{i = |G|} \text{score}(s_i) * p_i
\end{displaymath}

where $p_i \in \{0, 1\}$ are binary indicators, where $p_i = 1$ indicates sentence $s_i$ should be included in the final summary.

To avoid redundancy, the following set of constraints is imposed:

\begin{displaymath}
p_i + p_{j} \leq 1 \forall i \neq j \land sim(s_i, s_j) > t
\end{displaymath}

Following \citet{banerjee}, we set $t = 0.8$ and use cosine similarities between vector representations of the two sentences for similarity computation.

A second set of constraints ensures that only one sentence from the same cluster may be included in the final summary:

\begin{displaymath}
\sum_{s_i \in C} p_i \leq 1 \forall C \in \bar{C}
\end{displaymath}

Finally, we also need to add an appropriate length constraint. We introduce a per-day summary length constraint in sentences:

\begin{displaymath}
\sum_{i} p_i \leq k
\end{displaymath}


\section{Adaption to timeline summarization}

As mentioned in the introduction, we can use the system described above as a TLS system by running a date-selection algorithm and then running the MDS system on the documents of each day.

While there are sophisticated solutions for the problem (see, for example \citet{tran-datesel}, who model the problem as a random walk on a graph of dates), investigating date selection is not the primary focus of this work.
We thus reach for a simpler importance metric, which relies on the intuition that a date is important if it is mentioned often in the corpus and select the $k$ most frequently mentioned dates for inclusion in the timeline.

%Date selection has been investigated both as part of end-to-end TLS systems (see, for example \citet{trans}), as well as a standalone problem.
%In this work we opt for a simple variant of the above mentioned principle and rank each date by number of mentions of the date in the corpus.

\chapter{System}

\section{Architectural Overview}

Our approach can broadly be divided into four distinct subproblems, generalizing the setup of \citet{banerjee}.
We take as input a collection of sentences $U$ from a set of documents $\bar{D}$.
The start with the \textbf{clustering step} where we seek to partition the sentences $U$ into a set of clusters $\bar{C}$. Each cluster should ideally represent one distinct event.
The second step is to generate a set of \textbf{candidate sentences} $G$ that each concisely summarize the content of a single cluster. Note that many more candidates are generated than can be included in the final summary. Clusters also need to be assigned a timestamp, to determine which date the candidates summarize.
In the third step each generated sentence $s \in G$ is assigned a \textbf{score}, which represents its importance for the summary.
Finally, we must \textbf{select} a number of sentences from $G$ to be included a timeline, so that the resulting summary is both informative and readable.

The baseline systen fits also into this general scheme, where the number of selectable dates is constrained beforehand by the date selection algorithm.

In the rest of this section we describe the methods we have developed within this framework for TLS. We present new approaches to clustering, scoring and selection. We do not, however, change the sentence generation phase as described in section \ref{sec:baseline-generation}.

\section{Clustering}

While there is a large body of work on clustering algorithms available, the requirements of this application place some constraints on which algorithms we can consider. Sentences in one cluster must be both similar, so the compression can generate meaningful and correct sentences, as well as sufficiently different to allow for variations in the generated sentences, lest the system will just reproduce the original sentences.
Furthermore the system must automatically detect the number of clusters on a wide varity of datasets with different sizes and characteristics.
Choosing too few large clusters may make it impossible to generate a sufficient ammount of sentences to fill a timeline, while too many small clusters may not be diverse enough to allow for new sentences.
These factors make the clustering step of the system quite challenging and at the same time an essential factor in improving the performance of the system.

%\subsection{Document-based clustering}
%
%We can directly apply the clustering method of our reference MDS system as described in section \ref{sec:baseline-system} by treating each date %as an isolated clustering problem.
%Each date that has at least two articles in the corpus is treated as an independet clustering problem.
%We first determine the most-important document by finding the document which has the highest cosine similarity to the concatenation of all %documents in the date.
%Afterwards the sentences of the remaining documents are clustered to the most similar sentence of the most-important document, provided they have %a cosine similarity of $0.5$. Cosine similarity is computed based on a Tf-idf model fitted on the entire corpus of input documents.
%This setup corresponds to the best-performing system of \citet{banerjee}.
%
%One drawback of this clustering method is that it ignores possible date tagging on the sentence level and assumes instead that all sentences %refer to the publication date. 

\subsection{Agglomerative Multi-Clustering}

The document-based clustering approach detailed in section \ref{sec:baseline-clustering} assumes that all sentences in a document describe events on the same date, the document creation time. This assumption, however, ignores the more detailed date-tags that systems such as HeidelTime \citep{heideltime} can provide.
When moving to this more fine-grained date tagging level, we can no longer rely on documents to guide our clustering. Sentences that refer to one date may hail from many different documents and the sentences of one document can refer to many different dates.
Consequently, we can also no longer rely on one document to guide the number of clusters as in the baseline system. We can, however, partly avoid the resulting problem of having to choose the number of clusters by relaxing the problem so that one sentence may be included in more than one cluster.
This means that we do not have to decide into which cluster a given sentence fits best, but just whether its inclusion would benefit a given cluster or not.

The reason for this relaxation becomes clearer, if we analyse the effects of placing a sentence in a cluster it does not belong to versus not placing an imporant sentence in a cluster that would benefit from it.
In the first case, the generation step will probably produce a few nonsensical sentences, because it combines sentences which have very little in common.
It might also generate redudant sentences, because two clusters can have very similar content. Both problems, however, can conceivably be handled at the later selection stage.
The second problem, on the other hand, is much more difficult to correct at a later step. If a sentence carrying useful information is not included in a cluster, that information is essentially lost to the summarizer. It seems thus reasonable to choose a generation strategy that prefers making mistakes of the former kind instead of the latter.

Based on this observation, we formulate the following clustering strategy. We first partition all sentences $U$ of the input documents into sets $P_{d_1}, \dots P_{d_n}$, $d_1, \dots, d_n \in \{ date(s) | s \in U \}$ so that all sentences that refer to the same date are in one set. We follow prior work on TLS in assuming that the correct date tag for each sentence is the first complete date tag encountered in the sentence, or the document creation time, if there is no such tag.

We then consider each sentence $s$ in a partition $P_d$ and build a cluster based on each sentence.
To construct the cluster for sentence $s$, we find every sentence $s' \in P_d$ so that $\text{sim}(s, s') > 0.5$ and add $s'$ to the cluster induced by $s$.

We eliminate all clusters with only one member. We now have at most $|P_d|$ potential clusters for partition $P_d$.
Many of the remaining clusters are bound to be very similar. While this is not a fundamental problem for the system, besides complicating redundancy avoidance in later steps, it is undesirable from a performance perspective, because it leads to a lot of unnecessary duplication of candidate sentences in the generation step.
We thus eliminate any clusters, which is a subset of another cluster in the same partition. See algorithm \ref{alg:multi-clustering} for a sketch of the whole process.


 \begin{algorithm}
 \caption{Agglomerative Multi-Clustering}
 \label{alg:multi-clustering}
 \begin{algorithmic}
    \STATE Partition the input sentences into sets $P_d = \{ s \in U | date(s) = d \}$ for each date $d$ in the corpus
    \STATE $\bar{C} = \emptyset$
    \FOR{all sets $P_d$}
        \FOR{$s_1 \in P_d$}
            \STATE Let $C = \{  s_1 \}$
            \FOR{$s_2 \in P_d \setminus \{ s_1 \}$}
                \IF{$sim(s_1, s_2) > 0.5$}
                    \STATE $C \leftarrow C \cup \{ s_2 \}$
                \ENDIF
            \ENDFOR

            \IF{$|C| \geq 2$}
                \STATE $\bar{C} \leftarrow \bar{C} \cup \{ C \}$
            \ENDIF
        \ENDFOR

    \ENDFOR
    \STATE $\bar{C} \leftarrow \text{EliminateDuplicates}(\bar{C})$
    \RETURN $\bar{C}$

\end{algorithmic}
\end{algorithm}

\subsection{Graph-based Clustering}

Both the agglomerative and the document-based clustering methods have in common that they assume that each sentence has an unambigous date reference (either a date expression or the document creation date).
This assumption, however, is not always correct. Many date references are incomplete in that they do not refer to an exact date, but instead to a timespan. Take the following example from the crisis corpus: ''A hospital source and an activist reported 17 dead in Deraa , where protests began last month before spreading across the country.'' \footnote{Source: Crisis corpus, document 2011-04-08/100.htm.txt} % corpora/syria/articles/2011-04-08/100.htm.txt: (TABLE!)
Instead of refering to an exact date, the article instead contains an approximate reference to ''last month''. While this construction would be recognized by a time expression tagger, many traditional TLS systems would discard this reference, because it does not refer to an exact date.
Where it is handled, for example in the system of \citet{chieu}, it may be taken into account to determine the importance of other sentences, but it not usually assigned an exact date.
A small corpus study on the timeline 17 and crisis corpora (see section \ref{sec:corpora}) presented in table \ref{tab:ambigous-date-ref} shows, that these constructions are not uncommon . %(TABLE!).
Handling them becomes even more attractive when we consider that \citet{datesel-graph} have argued that events which are referenced a long time after they have happened are likely to be very important, because they have apparently remained in collective memory. %(CITE!)
%In the same vein, it might be useful to try to resolve date assignment for sentences with multiple date references instead of just discarding all date references besides the first.

We tackle that problem by exploiting sentence similarities to the sentences in the referred to timespan to find the exact date the ambigous reference is likely to refer to. In order to do this, we cast the problem of sentence clustering as a graph-clustering problem, where nodes are candidate sentences and edges are preferences for two sentences to be clustered together. By restricting which edges are included in the graph based on the dates referenced in the sentences they connect, we can ensure that sentences only end up in the same cluster if their dates match.

We define the function $\text{daterefs}(s)$ to return the set of date references of sentence $s$. These references include all TimeML-expressions contained in the sentence of type ''date'' and the document creation time of the parent document of $s$.
We call a reference $r \in \text{daterefs}(s)$ exact, iff it corresponds to the resolution of the timeline, i.e. the smallest unit of time the timeline covers in one summary. Usually this resolution is one day. We assume the document creation times are always exact references. All other references are ''inexact''.

%Intuitively this encodes that a sentence that mentions one ore more exact dates will refer to one of these dates. A sentence with one or more inexact tags might refer to a point in time inside the mentioned timespan, or it might refer to events happening at the time of publication. A sentence without any tags will always assumed to refer to the date of publication.
Intuitively this procedure encodes the assumption that a sentence that refers to an event in the past will contain some sort of reference to the date of that event, otherwise it is likely to refer to an event that has happened on the date of publication.
Note that we do not assume the converse. Just because a sentence includes some sort of reference to the past, its focus does not necessarily have to be about an event in the past. Instead it might just contain these references as background info.
Take the following example sentence: ''A third company defended work that it performed on the deepwater oil well as ''accepted industry practice'' before last month's explosion.'' \footnote{Source: Timeline17 corpus, document 2010-05-10/216.htm.txt} 
While there is a reference to the past (''last month''), the event (''a third company defended work'') clearly did not happen then.
Naturally it would be possible to extend the number of possible date references and including the tags of surrounding sentences as well, to account for situations were a past event is described over multiple sentences and the date is not repeated. We do, however, not include such references to restrict the number of edges in the resulting graph to a computationally feasible number.
%Note that this definition is not set in stone. For example one may assume that an untagged sentence would be likely to refer to any date mentioned in the same document. However, constructing a graph for two many date references quickly becomes computationally expensive, so we restrict ourselves to the above mentioned defintion.

To deal with the problem of inexact references, we also need the notion of one date reference being contained by another, for example, the second calendar week of 2013 is contained in the month January of that year.
A date reference $r_1$ is contained in a date reference $r_2$ iff $r_1$ falls within the timespan covered by $r_2$, or both references reference the same timespan.

Using these notions, given a set of sentences $U$ we define a weighted, directed graph $H = (V, E)$ with weight function $w$ as follows:
\begin{align*}
V = U \\
E = \{ (s_1, s_2) \in V \times V \mid \exists r_1 \in \text{daterefs}(s_1), r_2 \in \text{daterefs}(s_2) \colon \\ r_2 \text{ contains } r_1 \land r_1 \text{ is exact} \} \\ % Inexact to inexact edges?
w(s_1, s_2) = \text{sim}(s_1, s_2)
\end{align*}

Note that while we introduce edges from inexact to exact references, we do not allow edges in the opposite direction. This means that while sentences with inexact references have a preference to be grouped with similar sentences with exact date references, this is not true the other way around.
This avoids that two sentences with different exact date references are grouped in the same cluster via the sentence with the inexact reference.

Having constructed the graph, we now need to find an appropriate clustering of the nodes in $H$. While we could theoretically use any algorithm that can cluster nodes in a directed graph, in this work we adopt Affinity Propagation for this task.

Affinity Propagation (AP) \citep{ap} is a clustering method that is based on the concept of message passing. It identifies a set of exemplars from the input instances which are the centers of the clusters. Each instance chooses its examplar based on its affinities to other points $s(i, k)$. A high affinity indicates a high preference to be clustered with the other point.
The total number of examplars is not fixed but is instead controlled by the diagonal of the similarity matrix $s(k, k)$, the \textit{preferences}. Larger values on the diagonal lead to the corresponding instance to be more likely to be an exemplar.
As we have no preference for the number of clusters, we set the diagonal to the median of similarities for each instance, as suggested by \citeauthor{ap}.
In each step of the algorithm, two matrices are updated, responsibility $r$ and availability $a$. The responsibility $r(i, k)$ indicates the degree to which the instance $i$ believes it should choose $k$ as an examplar over other potential examplars. It is updated according to the following rule:
\begin{displaymath}
r(i, k) \leftarrow s(i, k) - max_{k' \neq k}( a(i, k') + s(i, k') )
\end{displaymath}

The availability $a(i, k)$ indicates how appropriate it is for point $i$ to choose $k$ as an exemplar.
It is updated as follows for $i \neq k$:
\begin{displaymath}
a(i, k) \leftarrow \min 0, r(k, k) + \sum_{i' \not\in \{i, k\}} \max(0, r(i', k))
\end{displaymath}

For $k = i$ we have:
\begin{displaymath}
a(k, k) \leftarrow \sum_{i' \neq k} \max(0, r(i', k))
\end{displaymath}

The procedure is then repeated until convergence.

Affinity Propagation is well suited to our task, as it does not need the number of clusters to be specified beforehand. Furthermore it has been used for clustering in summarization before \citep{salient-updates-disaster}. Furthermore it can be implemented in a sparse manner so that each iteration of the AP algorithm is only linear in the number of similarities, instead of quadratic in the number of instances. Given the sparse nature of our graph, which is a result of most sentences refering to different dates, this is a very useful property.

We further introduce a requirement for the similarity of two sentences to be at least $0.2$ for two nodes to be connected. While this is not strictly required from a theoretical standpoint, it greatly speeds up computation of the clustering and reduces storage requirements for the graph.

As before, we compute the similarities using cosine similarity of the tf-idf vector representations of the sentences.

\subsubsection{Date assignment}

Clusters resulting from the method we described above are not associated with a clear date tag. We thus need to determine the appropriate date tag for a cluster $C$ after the clustering step. We choose a simple heuristic and count the number of times a date is mentioned in the corpus: $\bigcup_{s \in C} \{r \in daterefs(s) | r \text{ is exact}\}$. The date tag for the sentence is the tag with the highest frequency.

%\section{Candidate Generation}

%During candidate generation we largely rely on the same method and procedure that was used by \citet{banerjee} and outlined in section \ref{sec:baseline:details}. We do, however, change 

\section{Scoring} \label{sec:scoring}

Another avenue to improve the MDS system for timeline summarization is the scoring function.
In the following section we propose a number of scoring functions that seek to adapt the existing functions to TLS.
Following \citet{banerjee}, the final score of a sentence is the product of the values of the individual scoring functions.

\subsection{MDS Scores} \label{sec:mds-scores}

The scoring functions used in the baseline MDS system can naturally also be applied to the timeline summarization task.
We refer to the linguitic quality score as $f_{ling}$ and to the informativeness score as $f_{TR^{(cluster)}}$.
We refer to the normalization by sentence length in the original scoring function as $f_{length}(s) = 1 / |s|$.
%It should be noted, however, that we do not use the  apply $f_{TR_{cluster}}$ score in isolation  

\subsection{Global TextRank}

The MDS functions described in \ref{sec:mds-scores} score candidates based on their importance in relation to the sentence in their cluster.
However, this score is not informative about the importance of the sentence in relation to the summarization as a whole.
It is thus reliant on some kind of filtering of irrelevant clusters. \citet{banerjee} solve this problem by removing clusters that are small in relation to the number of documents in the corpus. This solution, however, is difficult to directly transfer to timeline summarization, because there is no natural reference point for which cluster is ''large enough'' to warrant inclusion in the timeline.

We thus seek to score the candiate sentences in relation to their importance for the whole summary, instead of the cluster.

A natural way to achieve this is to compute the TextRank scores on the concatenation of all documents in the corpus, instead of per cluser.
We refer to the score derived this way as $f_{TR^{(global)}}$.

\subsection{Temporalized TextRank}

While the global score gives an indication of the importance of a sentence in relation to the corpus, it does not respect that the importance of keywords may vary over time as the event develops.
We thus extend the TextRank scoring to be computed per date. We first collect all sentences that pertain to one date, as per their cluster membership, and then run the TextRank algorithm to obtain keyword scores for all non-stopwords. We call this per-date TextRank $TR^{(date)}$.

Previous research \citep{yan-trans, markert} has show that the sentence similarity based scoring methods can be improved, if similarity is not only computed for sentences with the same timestamp, but also for sentences with neighbouring timestamps as well. The influence of sentences that are further apart is discounted based on the distance of their timestamps.
We build on this idea to temporalize the TextRank algorithm by allowing the TextRank scores of a date to be influenced by neighbouring dates. The influence of neighbouring scores is determined by the distance in days between the two dates and a soft cutoff function $g: \mathbb{N} \rightarrow \mathbb{R}$ that decreases with large date differences.
The final TextRank score is the average score of a word, weighted by these factors.

Let $V$ be the vocabulary of the candidate sentences and $E$ the set of all dates refered to in the corpus. We define the temporalized TextRank score $TR_{temp}(w), w \in V$ at the date $d$ follows:

\begin{displaymath}
    TR^{(temp)}_d(w) = \frac{\sum_{d' \in E} TR^{(date)}_{d'}(w) * g(|d - d'|)}{\sum_{d' \in E} g(|d - d'|)}
\end{displaymath}

Following prior work \citep{markert}, we use $g(x) = \frac{1}{\sqrt{x + 1}}$.

Using these values, we can define the scoring function $f_{TR^{(date)}}(s) = \sum_{w \in s} TR^{(temp)}_{\text{date}(s)}(w)$ for $TR^{(temp)}$, where $\text{date}(s)$ refers to the date that was assigned to the candidate $s$ via the cluster it was generated from.
The defintion is analogous for $f_{TR^{(date)}}$.

\subsection{TLS Inspired Scores} \label{sec:tls-scores}

In addition to the improved MDS scores it might also be worthwile to include scoring functions that have been shown to be useful especially in the TLS task.

One such metric is the number of times a date is referenced in the corpus \citep{markert, datesel-graph}. It is motivated by the intuition that a date that is mentioned often in the corpus is probably important for the event in question. We define the date reference score $f_{\text{dateref}}$ as the number of times the date of the sentence was mentioned devided by the number of sentences in the corpus $|U|$ for normalization:

\begin{displaymath}
f_{\text{dateref}}(s) = \frac{\text{refcount}(\text{date}(s))}{|U|}
\end{displaymath}

Another metric that has been used in both TLS \citep{chieu} and MDS (for example \citet{lexrank}) is the cosine similarity to the other sentences in the corpus.
Applying this metric directly to our problem is computationally expensive, because it requires $|U| * |G|$ similarity computations and $|G|$ may include more than a million sentences.

A cheaper alternative is to use the size of the cluster a sentences stems from as a proxy. Clusters are assigned based on similarity and large clusters thus indicate that a sentence is very similar to many other sentences.

We define the $f_{\text{cluster size}}$ function as follows:

\begin{displaymath}
f_{\text{cluster size}}(s) = \frac{|\text{cluster}(s)|}{\max_{C \in \bar{C}}{|C|}}
\end{displaymath}

We include the normalization by maximum cluster size to squash the score in the general order of magnitude of the other scores and to account for the fact that what is considered ''large'' for a cluster may vary based on the corpus.

%This notion can be very naturally encoded in our framework by considering cluster size. % What about multiple clusters for the same date as a scoring metric?

\subsection{Path Weight}

While we already consider path weight in the generation stage, this does not always guarantee that proposed candidates were generated by a low weight path. If the generation graph is small enough for all or most paths to be generated, then even low weight paths might be included in the candidate set.
To account for this, we introduce a score based on path weight. To align with the notion common in other scores that larger values are preferable, we invert the path weight:

\begin{displaymath}
f_{path}(s) = (\text{pathweight}(s) + 1)^{-1}
\end{displaymath}


\section{Selection using Submodular Functions}

%\subsection{Integer Linear Programming}

%Formulating an Integer Linear Program (ILP) for infering an optimal summary given some sentences and associated importance scores is a common %approach in document summarization. %(CITE!)
%
%Given the set of generated sentences $G$ and the scoring function $f_{score}$ (s. \ref{sec:scoring}) we formulate the following maximization %problem.
%
%\begin{displaymath}
%\text{maximize} \sum_{g \in G} p_g * f_{score}(g)
%\end{displaymath}
%
%where $p_g$ is a boolean indicator variable indicating whether the generated sentence $g$ should be included into the summary or not.
%
%Following \citet{banerjee}, we also include two sets of constraints into the problem.
%The first constraint ensures non-redundancy by enforcing that of any two sentences with a similarity greather than some threshold $t$ only %one may be included in a summary:
%
%\begin{displaymath}
%p_g + p_{g'} \leq 1 \forall g \neq g' \land sim(g, g') > t
%\end{displaymath}
%
%Following \citet{banerjee}, we set $t = 0.8$ and use cosine similarities between vector representations of the two sentences for similarity %computation.
%The vector representations are derived from a tf-idf model fitted on the entire collection of input documents.
%
%The second set of constraints ensures that only one sentence from the same cluster may be included in the final summary:
%
%\begin{displaymath}
%\sum_{g \in C} p_g \leq 1 \forall C \in \bar{C}
%\end{displaymath}
%
%Finally, we also need to add a length constraint. We introduce a per-day summary length constraint:
%
%\begin{displaymath}
%\sum_{g} p_g \leq k
%\end{displaymath}


As \citet{mcdonald} has shown, running ILP to find a summarization is challenging even for a moderate amount of textual units for the purpose of MDS quickly becomes infeasible.
This problem is aggrevated on TLS corpora, as they can be an order of magnitude larger than traditional MDS systems. For example, while the DUC2004 data has at most 10 documents per input cluster, the crisis corpus we use for evaluation has $\approx 4000$ news stories per topic. Additionally, generating a huge number of candidate sentences can increase the number of sentences even more, further limiting the usefulness of ILPs.
To alleviate this problem, we only run the solver for the set of candidates generated for a single date and filter the input to make the problem feasible. As a consequence the system with the ILP can not be run without external date selection. Furthermore, inter-date redundancy can not be prevented by the constraints.

%\subsection{Submodular Functions}

We thus turn to a computationally more efficient way to select sentences.
\citet{markert} have demonstrated that the objective for timeline summarization can be cast as a monotone, positive submodular function whose optimum can be reasonably well approximated by a greedy algorithm.

The submodularity property is defined as follows:
 Let $G$ be a finite set. A function $f: 2^{G} \rightarrow \mathbb{R}$ is submodular, if it fulfills the diminishing returns property: for $A \subset B \subseteq G$ and some $e \in G \setminus B$ $f(A) \leq f(B)$ implies $f(A \cup e) - f(A) \geq f(B \cup \{e\}) - f(\{B\})$.

A submodular function for TLS is a function $f: 2^{G} \rightarrow \mathbb{R}$, where $G$ is the set of sentences available for summarization.
The objective of the summarization algorithm is then to find a set $T \subseteq G$ that maximizes $f$.

In our case we adapt a function from a submodular MDS system by \citet{lin+blimes} designed to avoid redundancy, that was also used by \citet{markert}:

\begin{displaymath}
f_{\text{submod}}(X) = \sum_{i = 1}^{m} \sqrt{\sum_{s \in G \cap P_i} f_{\text{score}}(s)}
\end{displaymath}

where $P_1, ..., P_m$ form a partitioning of the generated sentences into $m$ disjoint sets. Including more than one sentence from the same cluster results in diminishing returns for further sentences from the same cluster, because the square root function will saturate.
By partitioning semantically similar sentences into the same partition, this property can be used to avoid redundancy in the summary.
Following \citet{lin+blimes} we obtain the clustering by k-means clustering on the tf-idf vectors of the generated sentences. We do, however, modify the number of clusters we generate. Both \citet{lin+blimes} and \citet{markert} use $0.2 * |U|$ clusters. The analogue for our setting would be $0.2 * |G|$ clusters. This ignores, however, that the generation process naturally produces a much larger number of very similar sentences. We instead choose to set the number of clusters based on the average number of sentences in the daily summaries multiplied with the number of dates to summarize times 2.

 For the usual timeline length constraints, length of daily summaries and number of dates in the timeline, \citet{markert} have shown that a greedy algorithm can solve the problem with a guaranteed lower bound of $1 / (k + 1)$ of the optimal value, where $k$ is the maximum number of sentences in a daily summary.
 In addition to these constraints, we also introduce a cluster constraint as mentioned in section \ref{sec:baseline-selection}, to ensure no two sentences from the same cluster are included in the summary:

\begin{displaymath}
|C \cap T| \leq 1 \forall C \in \bar{C}
\end{displaymath}

Note that with this selection algorithm it is no longer necessary to limit the number of generated sentences to just 200, so we move the limit to 2500 sentences.
We do not completely eliminate the limit, because the huge number of possible paths in a graph makes it infeasible to always generate all simple paths. The cubic complexity of the k-shortest simple paths algorithm further limits the number of sentences we can generate in an acceptable timeframe.

\chapter{Experimental Setup}

\section{Corpora} \label{sec:corpora}

We use two different timeline corpora for all our experiments, the \textit{Timeline 17} corpus \citet{tran-tl17} and the \textit{crisis} corpus. They both have a similar structure. Each covers a set of different events and has between one and five(?) timelines per event that were crawled from large news websites. The timelines all have different lengths and often cover different periods of time. The corpus also ships with a set of news documents that were crawled from news articles relating to the timeline. All articles were stripped of HTML tags and annotated with the time they were published, the document creation time. (CHECK!)

While the \textit{crisis} dataset focuses on several uprisings resulting from the arab spring, the timeline 17 dataset covers a more diverse range of topics from natural disasters to the death of Michael Jackson and subsequent legal procedings. See table \ref{tab:corpus-topics} for an overview over the events covered in the two corpora.

\section{Preprocessing}

Our approach needs all input documents to be annotated with part-of-speech tags, which we obtain by running the POS-tagger included in the Stanford CoreNLP suite of NLP tools \citep{stanford-corenlp}. The suite also takes care of proper tokenization and sentence splitting for us.
To obtain sentence level date annotations, we run the HeidelTime tagger \citet{heideltime}. Heideltime is a rule based system that tags date expressions in a corpus and labels them according to the TimeEX3 annotation standard.

\section{System Configurations}



\chapter{Automatic Evaluation}

\section{Setup}

We follow the evaluation setup of \citet{markert} and parameterize our timelines by two properties: Number of dates and number of sentences per date. Both are set from the reference timelines, where the number of sentences per date is the rounded-down average of sentences per date in the reference timeline.
As \citeauthor{markert} we generate one system timeline per reference timeline with the appropriate parameters, to account for the huge differences in length and verbosity of the timelines.

We evaluate the performance of our systems using the evaluation framework of \citet{tilse}, which is based on the ROUGE \citep{rouge} metric.

Specifically, we use ROUGE-1 and ROUGE-2 metrics, which are defined as follows:

\begin{displaymath}
    ROUGE-R(s, r) = 
\end{displaymath}

The concat score is obtained by concatenating all date summaries in both system and reference timeline and computing the ROUGE metric on the pair.
The agree score is obtained by treating each date summary individually and computing the score per date.
Finally the align m:1 metric aligns summaries for different dates based on the time difference between the tag of the reference timeline summaries and the contents of the daily summaries. The final summary score is the ROUGE score between aligned dates discounted proportionally to the time difference between their date tags.

Note that we compute the per-topic macro average of all metrics, not the average across all reference timelines.

\section{Results}

\chapter{Manual Evaluation}

\section{Setup}

\section{Results}

\chapter{Error Analysis}

\section{Common Errors in Clustering}

\section{Common Errors in Candidate Generation}

%\section{Problems with Current Evaluation Metrics}

\chapter{Conclusion}

\bibliographystyle{plainnat}
\bibliography{bib} 

\end{document}
